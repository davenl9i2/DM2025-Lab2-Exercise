{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"collapsed_sections":["4e5eiVLOYTp5"],"provenance":[]},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"oldHeight":594.85,"position":{"height":"40px","left":"723px","right":"20px","top":"80px","width":"250px"},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"varInspector_section_display":"none","window_display":true},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13121339,"sourceType":"datasetVersion","datasetId":8311974},{"sourceId":13712634,"sourceType":"datasetVersion","datasetId":8723499}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Kaggle Environment Setup:**","metadata":{}},{"cell_type":"code","source":"# Installing the missing libraries\n# This command has worked so far somewhat reliably, you can also try the other ones if it doesn't, and if it's still not working contact the TAs\n!pip3 install chromadb langchain_community langchain_google_genai langextract ollama pymupdf","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:49:01.459734Z","iopub.execute_input":"2025-11-14T09:49:01.460206Z","iopub.status.idle":"2025-11-14T09:50:06.163168Z","shell.execute_reply.started":"2025-11-14T09:49:01.460168Z","shell.execute_reply":"2025-11-14T09:50:06.161503Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Install the missing libraries in the kaggle environment manually like this if needed\n!pip3 install chromadb==1.1.0 langchain_community==0.3.29 langchain_google_genai==2.1.12 langextract==1.0.9 ollama==0.5.4 pymupdf==1.26.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:50:06.165377Z","iopub.execute_input":"2025-11-14T09:50:06.165672Z","iopub.status.idle":"2025-11-14T09:50:19.287636Z","shell.execute_reply.started":"2025-11-14T09:50:06.165653Z","shell.execute_reply":"2025-11-14T09:50:19.286397Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# In case of the library test not working you can uncomment this line and run the full library installation with the versions\n# that worked when testing the lab\n!pip3 install python-dotenv==1.1.1 google-genai==1.21.1 langextract==1.0.9 gensim==4.3.3 tensorflow==2.18.0 tensorflow-hub==0.16.1 keras==3.8.0 ollama==0.5.4 langchain==0.3.27 langchain_community==0.3.29 langchain_core==0.3.76 langchain-google-genai==2.1.12 beautifulsoup4==4.13.4 chromadb==1.1.0 gradio==5.31.0  scikit-learn==1.2.2 pandas==2.2.3 numpy==1.26.4 matplotlib==3.7.2 plotly==5.24.1 seaborn==0.12.2 nltk==3.9.1 umap-learn==0.5.7 pymupdf==1.26.4","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:50:19.288702Z","iopub.execute_input":"2025-11-14T09:50:19.288930Z","iopub.status.idle":"2025-11-14T09:50:53.079274Z","shell.execute_reply.started":"2025-11-14T09:50:19.288908Z","shell.execute_reply":"2025-11-14T09:50:53.077655Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# test code for environment setup\n# import library\nfrom bs4 import BeautifulSoup\nimport chromadb\nimport dotenv\nimport gradio\nfrom google import genai\nimport jupyter\nimport langchain\nfrom langchain_community import utils\nfrom langchain_core import prompts\nfrom langchain_google_genai import chat_models\nimport langextract\nimport matplotlib\nimport nltk\nimport numpy\nimport pandas\nimport plotly\nimport pymupdf\nimport seaborn\nimport sklearn\nimport umap\n\n%matplotlib inline","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:50:53.080316Z","iopub.execute_input":"2025-11-14T09:50:53.080640Z","iopub.status.idle":"2025-11-14T09:51:49.561373Z","shell.execute_reply.started":"2025-11-14T09:50:53.080613Z","shell.execute_reply":"2025-11-14T09:51:49.560028Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"**Table of contents**<a id='toc0_'></a>    \n- [Data Mining Lab 2 - Bonus](#toc1_)    \n  - [Before Starting](#toc1_1_)    \n  - [**1. Large Language Models (LLMs)** - Expanded explanation](#toc1_2_)    \n    - [Programmatic Access vs. Web Interfaces:](#toc1_2_1_)    \n    - [Optional Notebook Material for Exploration - Open Source LLMs:](#toc1_2_2_)    \n      - [Parameters and Quantization](#toc1_2_2_1_)    \n    - [Proprietary LLMs: The Gemini API](#toc1_2_3_)    \n    - [Interacting with the Gemini API](#toc1_2_4_)    \n    - [**1.1 Multi-Modal Prompting**](#toc1_2_5_)    \n      - [**1.1.1 Text + Images**](#toc1_2_5_1_)    \n        - [**1.1.1.1 Basic Image Understanding**](#toc1_2_5_1_1_)    \n        - [**>>> Bonus 1 (Take home):**](#toc1_2_5_1_2_)    \n        - [**1.1.1.2 Segmentation**](#toc1_2_5_1_3_)    \n        - [**>>> Bonus 2 (Take home):**](#toc1_2_5_1_4_)    \n      - [**1.1.2 Text + Video**](#toc1_2_5_2_)    \n        - [**1.1.2.1 Uploading local video:**](#toc1_2_5_2_1_)    \n        - [**1.1.2.2 Using YouTube video:**](#toc1_2_5_2_2_)    \n      - [**1.1.3 Text + Audio**](#toc1_2_5_3_)    \n        - [**>>> Bonus 3 (Take home):**](#toc1_2_5_3_1_)    \n    - [**1.2 Tool Calling:**](#toc1_2_6_)    \n      - [General Idea](#toc1_2_6_1_)    \n        - [**>>> Bonus 4 (Take home):**](#toc1_2_6_1_1_)    \n    - [**1.3 Information Extraction and Grounding:** - Expanded explanation](#toc1_2_7_)    \n      - [**`langextract`: A Library for Grounded Extraction**](#toc1_2_7_1_)    \n        - [**1.3.1 Using a video from youtube:**](#toc1_2_7_1_1_)    \n        - [**>>> Bonus 5 (Take home):**](#toc1_2_7_1_2_)    \n\n<!-- vscode-jupyter-toc-config\n\tnumbering=false\n\tanchor=true\n\tflat=false\n\tminLevel=1\n\tmaxLevel=6\n\t/vscode-jupyter-toc-config -->\n<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->","metadata":{}},{"cell_type":"markdown","source":"# <a id='toc1_'></a>[Data Mining Lab 2 - Bonus](#toc0_)\nThis is the bonus points' section for lab 2 phase 2.\n","metadata":{"id":"uuutyCx4YTpX"}},{"cell_type":"markdown","source":"## <a id='toc1_1_'></a>[Before Starting](#toc0_)\n\n**Make sure you have installed all the required libraries and you have the environment ready to run this lab.**\n    ","metadata":{}},{"cell_type":"markdown","source":"---\n## <a id='toc1_2_'></a>[**1. Large Language Models (LLMs)** - Expanded explanation](#toc0_)\n\nBefore we start we strongly suggest that you watch the following video explanations so you can understand the concepts that we are gonna discuss about extra LLMs concepts: \n\n1. [What Are Vision Language Models? How AI Sees & Understands Images](https://www.youtube.com/watch?v=lOD_EE96jhM)\n2. [How do Multimodal AI models work? Simple explanation](https://www.youtube.com/watch?v=WkoytlA3MoQ)\n3. [Multi-Modal LLMs for Image, Sound and Video](https://www.youtube.com/watch?v=_sGwL6RAsUc&t=1137s)\n4. [What is Tool Calling? Connecting LLMs to Your Data](https://www.youtube.com/watch?v=h8gMhXYAv1k)\n\n`These videos can help you get a better grasp on these additional concepts of LLMs covered in the bonus if you were not familiar before.`\n\n**So now let's start with the bonus content of Lab 2 Phase 2.**\n\nLarge Language Models (LLMs) are AI models trained on vast text data to understand and generate human language. Models like GPT and BERT excel at tasks like translation, summarization, and sentiment analysis due to their deep learning techniques and large-scale training. Recently these models got popular with the rise of Open-AI's ChatGPT and their different models, showcasing the potential of these models in a lot of aspects of our current society.\n\nOpen-source LLMs are cost-effective and customizable, with strong community support, but may underperform compared to proprietary models (Gemini, ChatGPT, Claude, etc.) and require technical expertise to manage. Proprietary LLMs offer superior performance, ease of use, and regular updates, but can be costly, less flexible, and create dependency on external providers for ongoing access and updates.\n\nOf course. Here is a revised version that frames the explanation objectively, focusing on the practical advantages for a student working on data mining exercises and projects.\n\n### <a id='toc1_2_1_'></a>[Programmatic Access vs. Web Interfaces:](#toc0_)\n\nFor this data mining lab, we will interact with LLMs using code (programmatic access) rather than a web-based chatbot. This approach provides several direct advantages for completing your exercises and projects:\n\n*   **Process Datasets Automatically**: Instead of manually copying and pasting individual data points, you can use a simple `for` loop in your code to send every row of a dataset to the model. This allows you to perform tasks like sentiment analysis or data extraction on hundreds or thousands of records efficiently.\n\n*   **Get Structured, Usable Output**: A chatbot provides conversational text that is difficult to use in analysis. With an API, you can command the model to return its output in a specific **JSON format**. This structured data can be directly and reliably loaded into a **pandas DataFrame**, allowing you to immediately continue with your data analysis and visualization tasks.\n\n*   **Ensure Consistent and Reproducible Results**: Scientific and data mining work must be reproducible. By setting parameters like `temperature=0.0` in your code, you make the model's output deterministic. This means you will get the exact same result every time you run your script with the same input, which is essential for debugging your code and validating your findings.\n\n*   **Maintain Data Privacy**: If you work with sensitive or private datasets for a project, uploading that data to a public web interface is not secure. Using an API provides better data governance, and running a model locally with a tool like Ollama ensures your data **never leaves your computer**.\n\n### <a id='toc1_2_2_'></a>[Optional Notebook Material for Exploration - Open Source LLMs:](#toc0_)\n\nFor students interested in running models locally, the optional notebook `DM2025-Lab2-Optional-Ollama.ipynb` explores using Ollama ([Ollama GitHub Link](https://github.com/ollama/ollama)). Ollama is a powerful library that simplifies running a wide range of open-source LLMs. In that notebook, we will use the **Gemma 3 (4B)** for multi-modal tasks and advanced text-based tasks, **Gemma 3 (270M version)** for simple text-based tasks, **Llama 3.2 (1B version)** for tool calling tasks, and **embedding-gemma (300M params)** to generate embeddings from text data. The Gemma models come from Google's family of models, while Llama 3.2 comes from Meta's family, all of them open source.\n\nTo use these models effectively, an **NVIDIA GPU with at least 4 GB of VRAM is recommended**. If you don't have the required hardware, you can run the examples in Kaggle or other cloud notebooks with their free GPU usage tiers activated.\n\nYou can explore the variety of models available through Ollama here:\n\n![pic10.png](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/pics/pic10.png?raw=True)\n\n#### <a id='toc1_2_2_1_'></a>[Parameters and Quantization](#toc0_)\n\nIn the image, you'll notice that models have a certain number of **parameters**. These are the internal variables the model learns during training, which store its knowledge. A higher parameter count generally leads to a more capable model but also increases its size and the computational resources needed to run it, especially VRAM.\n\nThis is where **quantization** becomes crucial. Quantization is a technique to reduce the memory footprint of a model by converting its parameters from high-precision data types (like 32-bit floating-point numbers) to lower-precision ones (like 8-bit integers). This process makes the model smaller and faster, significantly lowering VRAM usage, though it can sometimes cause a minor reduction in accuracy. Thanks to quantization, a 4-billion parameter model like Gemma 3 4B can be run on consumer-grade GPUs more efficiently.\n\n### <a id='toc1_2_3_'></a>[Proprietary LLMs: The Gemini API](#toc0_)\n\nFor the main exercises in this lab, we will use **the Gemini API**. This approach offers several advantages over running local open-source models, such as access to state-of-the-art model performance without needing specialized hardware. While the API has usage limits (rate limits and token quotas), it provides a generous **free tier** that is more than sufficient for our exercises.\n\n\n![pic13.png](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/pics/pic13.png?raw=True)\n\n![pic14.png](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/pics/pic14.png?raw=True)\n\nWe will primarily use the **Gemini 2.5 Flash-Lite** (`gemini-2.5-flash-lite`) model. As shown in the rate limit table, this model is optimized for high-frequency tasks and offers a high request-per-day limit of 1,000, making it ideal for completing the lab exercises without interruption.\n\nStudents are encouraged to explore other models available through the API but should remain mindful of their respective usage limits. For instance:\n*   **Gemini 2.5 Pro** is a more powerful model but has a lower daily request limit of 100.\n*   The **Gemma 3** model available via the API offers an impressive 14,400 requests per day, providing another excellent alternative for experimentation.\n\nPlease be aware of your usage limits as you work through the exercises to ensure you do not get rate-limited.\n\n[Gemini Documentation](https://ai.google.dev/gemini-api/docs)\n\n[Gemini Rate Limits](https://ai.google.dev/gemini-api/docs/rate-limits)\n\n[Description of Gemini Models](https://ai.google.dev/gemini-api/docs/models)","metadata":{}},{"cell_type":"markdown","source":"---\n\n### <a id='toc1_2_4_'></a>[Interacting with the Gemini API](#toc0_)\n\nThe code cell below contains the primary function, `prompt_gemini`, that we will use throughout this lab to communicate with the Gemini API. It's designed to be a flexible wrapper that handles the details of sending a request and receiving a response.\n\nBefore you run the exercises, here are the key things you need to understand in this setup:\n\n*   **API Key Configuration**: The script loads your API key from a `.env` file located in the `./config/` directory. **You must create this file and add your API key** like this: `GOOGLE_API_KEY='YOUR_API_KEY_HERE'`. This is a security best practice to keep your credentials out of the code.\n\n*   **Global Settings**: At the top of the script, you can find and modify several important defaults:\n    *   `MODEL_NAME`: We've set this to `\"gemini-2.5-flash-lite\"`, but you can easily switch to other models like `\"gemini-2.5-pro\"` to experiment.\n    *   `SYSTEM_INSTRUCTION`: This sets the model's default behavior or persona (e.g., \"You are a helpful assistant\"). You can customize this for different tasks.\n    *   `SAFETY_SETTINGS`: For our academic exercises, these are turned off to prevent interference. In real-world applications, you would configure these carefully.\n\n*   **The `prompt_gemini` function**: This is the main tool you will use. Here are its most important parameters:\n    *   `input_prompt`: The list of contents (text, images, etc.) you want to send to the model.\n    *   `temperature`: Controls the randomness of the output. `0.0` makes the output deterministic and less creative, while a higher value (e.g., `0.7`) makes it more varied.\n    *   `schema`: A powerful feature that allows you to specify a JSON format for the model's output. This is extremely useful for structured data extraction.\n    *   `with_tokens_info`: If set to `True`, the function will also return the number of input and output tokens used, which is helpful for monitoring your usage against the free tier limits.\n\nIn the following exercises, you will call this function with different prompts and configurations to solve various tasks.\n\nIf needed, you can also check some tutorials on how a python function works: [Python Functions Tutorial](https://realpython.com/defining-your-own-python-function/)","metadata":{}},{"cell_type":"code","source":"import os\nfrom google import genai\nfrom google.genai import types\nfrom kaggle_secrets import UserSecretsClient\n\nuser_secrets = UserSecretsClient()\n\n\n# System instruction that can dictate how the model behaves in the output, can be customized as needed\nSYSTEM_INSTRUCTION = (\n        \"You are a helpful assistant\"\n    )\n\n# Max amount of tokens that the model can output, the Gemini 2.5 Models have this maximum amount\n# For other models need to check their documentation \nMAX_OUTPUT_TOKENS = 65535\nMODEL_NAME = \"gemini-2.5-flash-lite\" # Other models: \"gemini-2.5-pro\", \"gemini-2.5-flash\"; Check different max output tokens: \"gemini-2.0-flash\" , \"gemini-2.0-flash-lite\" \n\n# We disable the safety settings, as no moderation is needed in our tasks\nSAFETY_SETTINGS = [\n    types.SafetySetting(\n        category=\"HARM_CATEGORY_HATE_SPEECH\", threshold=\"OFF\"),\n    types.SafetySetting(\n        category=\"HARM_CATEGORY_DANGEROUS_CONTENT\", threshold=\"OFF\"),\n    types.SafetySetting(\n        category=\"HARM_CATEGORY_SEXUALLY_EXPLICIT\", threshold=\"OFF\"),\n    types.SafetySetting(\n        category=\"HARM_CATEGORY_HARASSMENT\", threshold=\"OFF\")\n]\n\n# We input the API Key to be able to use the Gemini models\napi_key = user_secrets.get_secret(\"GOOGLE_API_KEY\")\nos.environ[\"GOOGLE_API_KEY\"] = api_key\nclient = genai.Client(api_key=api_key)\n\n# We also set LangExtract to use the API key as well:\nif 'GEMINI_API_KEY' not in os.environ:\n    os.environ['GEMINI_API_KEY'] = api_key\n\ndef prompt_gemini(\n        input_prompt: list,\n        schema = None,\n        temperature: float = 0.0,\n        system_instruction: str = SYSTEM_INSTRUCTION,\n        max_output_tokens: int = MAX_OUTPUT_TOKENS,\n        client: genai.Client = client,\n        model_name: str = MODEL_NAME,\n        new_config: types.GenerateContentConfig = None,\n        with_tools: bool = False,\n        with_parts: bool = False,\n        with_tokens_info: bool = False\n    ):\n        try:\n            # If we need a JSON schema we set up the following\n            if schema:\n                generate_content_config = types.GenerateContentConfig(\n                    temperature=temperature,\n                    system_instruction=system_instruction,\n                    max_output_tokens=max_output_tokens,\n                    response_modalities=[\"TEXT\"],\n                    response_mime_type=\"application/json\",\n                    response_schema=schema,\n                    safety_settings=SAFETY_SETTINGS\n                )\n            # If there is no need we leave it unstructured\n            else:\n                generate_content_config = types.GenerateContentConfig(\n                    temperature=temperature,\n                    system_instruction=system_instruction,\n                    max_output_tokens=max_output_tokens,\n                    response_modalities=[\"TEXT\"],\n                    safety_settings=SAFETY_SETTINGS\n                )\n            \n            # We add a different custom configuration if we need it\n            if new_config:\n                generate_content_config = new_config\n            \n            # For some tasks we need a more specific way to add the contents when prompting the model\n            # So we need custom parts for it sometimes from the \"types\" objects\n            if with_parts:\n                response = client.models.generate_content(\n                    model=model_name,\n                    contents=types.Content(parts=input_prompt),\n                    config=generate_content_config,\n                )\n            # In the simplest form the contents can be expressed as a list [] of simple objects like str and Pillow images\n            else:\n                response = client.models.generate_content(\n                    model=model_name,\n                    contents=input_prompt,\n                    config=generate_content_config,\n                )\n\n            if with_tools:\n                # print(response)\n                # Include raw response when function calling\n                completion = response\n                if with_tokens_info:\n                    log = {\n                        \"model\": model_name,\n                        \"input_tokens\": response.usage_metadata.prompt_token_count,\n                        \"output_tokens\": response.usage_metadata.candidates_token_count,\n                    }\n                    return completion, log\n                return completion\n            else:\n                completion = response.text\n                if with_tokens_info:\n                    log = {\n                        \"model\": model_name,\n                        \"input_tokens\": response.usage_metadata.prompt_token_count,\n                        \"output_tokens\": response.usage_metadata.candidates_token_count,\n                    }\n                    # Return the text response and logs (if selected)\n                    return completion, log\n                return completion\n        except Exception as e:\n             print(f\"Error occurred when generating response, error: {e}\")\n             return None","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:49.564241Z","iopub.execute_input":"2025-11-14T09:51:49.565435Z","iopub.status.idle":"2025-11-14T09:51:49.953384Z","shell.execute_reply.started":"2025-11-14T09:51:49.565408Z","shell.execute_reply":"2025-11-14T09:51:49.952387Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n### <a id='toc1_2_5_'></a>[**1.1 Multi-Modal Prompting**](#toc0_)\n\nMulti-modal prompting involves using input from multiple sources or modes, such as text, images, video or audio, to guide a model's response. It allows AI to process and generate information based on more than one type of input.\n\n---\n#### <a id='toc1_2_5_1_'></a>[**1.1.1 Text + Images**](#toc0_)\n\nGemini's multimodal capabilities allow it to perform sophisticated **image understanding**, turning unstructured visual data into analyzable information. Beyond basic tasks like captioning or answering questions about an image's content, it offers powerful features for data extraction:\n\n*   **Object Detection**: The model can identify objects and return their **bounding box coordinates**, providing structured location data.\n*   **Image Segmentation**: It can go a step further by providing a precise pixel-level **segmentation mask**, outlining the exact shape of detected objects.\n\nYou can provide single or multiple images by either **uploading a file** (for large images or reuse) or passing the image data **inline** within your API request.\n\nWe are going to explore how to prompt our model for basic image understanding and how to use the Gemini's segmentation capabilities.\n\nFor more information you can visit the following link: [Gemini's Image Understanding Documentation](https://ai.google.dev/gemini-api/docs/image-understanding)\n\n---\n##### <a id='toc1_2_5_1_1_'></a>[**1.1.1.1 Basic Image Understanding**](#toc0_)\n\nFor basic image understanding, the model receives both an image and a related text prompt. The image provides visual context, while the text gives additional guidance. The model uses both inputs to generate more accurate and contextually relevant responses, which is useful for tasks like image captioning, visual question answering, or content generation based on visual cues.","metadata":{}},{"cell_type":"markdown","source":"Let's look at the following images that are in the `pics` folder in the directory of this notebook:","metadata":{}},{"cell_type":"markdown","source":"![example1.png](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/pics/example1.png?raw=True)\n\nsource: https://cooljapan-videos.com/tw/articles/epe0y86g","metadata":{}},{"cell_type":"markdown","source":"![example2.jpg](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/pics/example2.jpg?raw=True)\n\nsource: https://www.istockphoto.com/photo/young-cat-scottish-straight-gm1098182434-294927481","metadata":{}},{"cell_type":"markdown","source":"We will use our selected Gemini model to request a description of the images:","metadata":{}},{"cell_type":"code","source":"from PIL import Image\nfrom IPython.display import display, Markdown\n\nimage_example_1 = Image.open('/kaggle/input/lab2-initial-data/pics/example1.png')\n\ninput_prompt_img = [\"What is this image about?\", image_example_1]\ntext_response_1, logs_1 = prompt_gemini(input_prompt = input_prompt_img, with_tokens_info = True)\ndisplay(Markdown(text_response_1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:49.954500Z","iopub.execute_input":"2025-11-14T09:51:49.954962Z","iopub.status.idle":"2025-11-14T09:51:53.196569Z","shell.execute_reply.started":"2025-11-14T09:51:49.954931Z","shell.execute_reply":"2025-11-14T09:51:53.195465Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(logs_1)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:53.197612Z","iopub.execute_input":"2025-11-14T09:51:53.197934Z","iopub.status.idle":"2025-11-14T09:51:53.205994Z","shell.execute_reply.started":"2025-11-14T09:51:53.197915Z","shell.execute_reply":"2025-11-14T09:51:53.204641Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\n\nimage_example_2 = Image.open('/kaggle/input/lab2-initial-data/pics/example2.jpg')\n\ninput_prompt_img_2 = [\"What is this image about?\", image_example_2]\ntext_response_2, logs_2 = prompt_gemini(input_prompt = input_prompt_img_2, with_tokens_info = True)\ndisplay(Markdown(text_response_2))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:53.207646Z","iopub.execute_input":"2025-11-14T09:51:53.208036Z","iopub.status.idle":"2025-11-14T09:51:55.820252Z","shell.execute_reply.started":"2025-11-14T09:51:53.208006Z","shell.execute_reply":"2025-11-14T09:51:55.819407Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(logs_2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:55.821245Z","iopub.execute_input":"2025-11-14T09:51:55.821469Z","iopub.status.idle":"2025-11-14T09:51:55.825609Z","shell.execute_reply.started":"2025-11-14T09:51:55.821454Z","shell.execute_reply":"2025-11-14T09:51:55.824905Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_5_1_2_'></a>[**>>> Bonus 1 (Take home):**](#toc0_)\n\nTry asking the model with one image of your choosing. Is the description accurate? Why?","metadata":{}},{"cell_type":"code","source":"# Answer here\n'''\nThe model can accurately identify and describe. It didn't miss a detail, also used several adjuctives to talk about the background of the image.\n'''\nfrom PIL import Image\n\nmy_image_example = Image.open('/kaggle/input/my-image/101.jpg')\n\nmy_input_prompt_img = [\"What is this image about?\", my_image_example]\nmy_text_response, my_logs = prompt_gemini(input_prompt = my_input_prompt_img, with_tokens_info = True)\ndisplay(Markdown(my_text_response))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:55.826134Z","iopub.execute_input":"2025-11-14T09:51:55.826298Z","iopub.status.idle":"2025-11-14T09:51:59.339935Z","shell.execute_reply.started":"2025-11-14T09:51:55.826286Z","shell.execute_reply":"2025-11-14T09:51:59.338130Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_5_1_3_'></a>[**1.1.1.2 Segmentation**](#toc0_)\n\nBeyond simply detecting objects, we can prompt Gemini to perform **image segmentation**, which involves predicting the precise pixel-level contour or \"mask\" for each object. [Gemini's Segmentation Documentation](https://ai.google.dev/gemini-api/docs/image-understanding#segmentation)\n\nTo understand this process, it's helpful to remember how computer vision models \"see\" images. A model doesn't perceive a picture; it sees a grid of numbers (pixels). Traditional segmentation models are trained specifically to classify every single pixel in this grid, assigning it to a class (e.g., \"cat,\" \"dog,\" \"background\"). This usually requires a highly specialized model trained on vast, manually labeled datasets.\n\nGemini offers a powerful alternative, performing this complex task without needing a specialized model. Its output, however, reflects these underlying computer vision principles. When asked to segment an image, Gemini returns a structured JSON list. For each detected object, it provides:\n\n*   **`\"box_2d\"`**: A bounding box with normalized coordinates `[y0, x0, y1, x1]`. Normalizing coordinates (scaling them to a standard range like 0-1000) is a common preprocessing step in machine learning that makes calculations consistent regardless of the original image size. Our code will need to \"de-normalize\" these coordinates back to the image's actual pixel dimensions.\n*   **`\"label\"`**: A text label identifying the object.\n*   **`\"mask\"`**: The segmentation mask itself, provided as a **base64 encoded PNG**. This encoding is a standard way to represent binary image data as a text string within a JSON response. The mask is not a simple binary outline but a **probability map**, where each pixel's brightness (0-255) represents the model's confidence that it belongs to the object.\n\nTherefore, to visualize the result, our code must perform a few key post-processing steps: decode the base64 string back into an image, resize this small mask to fit the dimensions of its bounding box, and finally, apply a confidence threshold (e.g., any pixel value > 127) to create the final binary mask.\n\nIf you are interested in the topic you can check these extra tutorials on image classification and segmentation: \n\n[PyTorch Image Classification](https://github.com/bentrevett/pytorch-image-classification)\n\n[Train your image classifier model with PyTorch](https://learn.microsoft.com/en-us/windows/ai/windows-ml/tutorials/pytorch-train-model)\n\n[Python | Image Classification using Keras](https://www.geeksforgeeks.org/machine-learning/python-image-classification-using-keras/)\n\n[Image Segmentation using Python's scikit-image module](https://www.geeksforgeeks.org/machine-learning/image-segmentation-using-pythons-scikit-image-module/)\n\n[Mediapipe | Image segmentation guide for Python](https://ai.google.dev/edge/mediapipe/solutions/vision/image_segmenter/python)\n\n**We are going to utilize the following image:**\n\n![example3.jpg](https://github.com/difersalest/DM2025-Lab2-Exercise/blob/main/pics/example3.jpg?raw=True)\n\nsource: https://www.pexels.com/photo/a-black-dog-and-orange-tabby-cat-fighting-on-the-road-10140894/","metadata":{}},{"cell_type":"code","source":"from PIL import Image, ImageDraw\nimport io\nimport base64\nimport json\nimport numpy as np\nimport os\nfrom pydantic import BaseModel\nfrom google.genai import types\nimport random\n\n# Schema for each element that is found inside the image\nclass Element(BaseModel):\n   label:str\n   box_2d: str\n   mask: str\n\ndef parse_json(json_output: str):\n  # Parsing out the markdown fencing\n  # Just as a double validation measure\n  lines = json_output.splitlines()\n  for i, line in enumerate(lines):\n    if line == \"```json\":\n      json_output = \"\\n\".join(lines[i+1:])  # Remove everything before \"```json\"\n      json_output = json_output.split(\"```\")[0]  # Remove everything after the closing \"```\"\n      break  # Exit the loop once \"```json\" is found\n  return json_output\n\ndef extract_segmentation_masks(image_path: str, items_to_detect: str, schema, model_name: str = \"gemini-2.5-flash-lite\", output_dir: str = \"segmentation_outputs\"):\n  # Colors to draw the bounding boxes, masks and labels\n  HIGH_CONTRAST_COLORS = [\n      (255, 0, 0),      # Red\n      (0, 255, 0),      # Lime\n      (0, 0, 255),      # Blue\n      (255, 255, 0),    # Yellow\n      (0, 255, 255),    # Cyan\n      (255, 0, 255),    # Magenta\n      (255, 165, 0),    # Orange\n      (0, 128, 128),    # Teal\n      (128, 0, 128),    # Purple\n      (255, 192, 203),  # Pink\n      (0, 250, 154),    # Medium Spring Green\n      (255, 215, 0),    # Gold\n  ]\n  # Load and resize image\n  im = Image.open(image_path)\n  im.thumbnail([1024, 1024], Image.Resampling.LANCZOS)\n\n  prompt = f\"\"\"\n  Give the segmentation masks for the {items_to_detect}.\n  Output a JSON list of segmentation masks where each entry contains the 2D\n  bounding box in the key \"box_2d\", the segmentation mask in key \"mask\", and\n  the text label in the key \"label\". Use descriptive labels.\n  \"\"\"\n  config = types.GenerateContentConfig(\n    response_mime_type=\"application/json\",\n    thinking_config=types.ThinkingConfig(thinking_budget=0) # set thinking_budget to 0 for better results in object detection\n  )\n  input_prompt = [prompt, im]\n  response = prompt_gemini(input_prompt = input_prompt, model_name = model_name, schema=schema, new_config=config)\n  print(f\"response: {response}\")\n  # Parse JSON response\n  items = json.loads(parse_json(response))\n\n  # Create output directory\n  os.makedirs(output_dir, exist_ok=True)\n\n  # Create a copy of the original image to draw all results on\n  final_result_image = im.convert(\"RGBA\")\n\n  # Process each mask\n  for i, item in enumerate(items):\n      # Get bounding box coordinates\n      box = item[\"box_2d\"]\n      y0 = int(box[0] / 1000 * im.size[1])\n      x0 = int(box[1] / 1000 * im.size[0])\n      y1 = int(box[2] / 1000 * im.size[1])\n      x1 = int(box[3] / 1000 * im.size[0])\n\n      # Skip invalid boxes\n      if y0 >= y1 or x0 >= x1:\n          continue\n\n      # Process mask\n      png_str = item[\"mask\"]\n      if not png_str.startswith(\"data:image/png;base64,\"):\n          continue\n\n      # Remove prefix\n      png_str = png_str.removeprefix(\"data:image/png;base64,\")\n      mask_data = base64.b64decode(png_str)\n      mask = Image.open(io.BytesIO(mask_data))\n\n      # Resize mask to match bounding box\n      mask = mask.resize((x1 - x0, y1 - y0), Image.Resampling.BILINEAR)\n\n      # Convert mask to numpy array for processing\n      mask_array = np.array(mask)\n\n      # Generate a random color for the current item\n      random_color = random.choice(HIGH_CONTRAST_COLORS)\n\n      # Create a new transparent image to draw the colored mask on\n      mask_overlay = Image.new('RGBA', final_result_image.size, (0, 0, 0, 0))\n      mask_draw = ImageDraw.Draw(mask_overlay)\n\n      # Draw the mask pixels onto the transparent overlay with the random color\n      # The alpha channel (transparency) is set to 150 (out of 255)\n      for y in range(y1 - y0):\n          for x in range(x1 - x0):\n              if mask_array[y, x] > 128:  # Threshold for mask\n                  mask_draw.point((x + x0, y + y0), fill=(*random_color, 150))\n\n      # Composite the colored mask onto the final result image\n      final_result_image = Image.alpha_composite(final_result_image, mask_overlay)\n\n      # Create a new draw object to draw the box and label on the updated image\n      final_draw = ImageDraw.Draw(final_result_image)\n\n      # Draw the bounding box\n      final_draw.rectangle([x0, y0, x1, y1], outline=random_color, width=3)\n\n      # Draw the label text\n      final_draw.text((x0, y0 - 15), item['label'], fill=random_color)\n\n      # Create overlay for this mask\n      overlay = Image.new('RGBA', im.size, (0, 0, 0, 0))\n      overlay_draw = ImageDraw.Draw(overlay)\n\n      # Create overlay for the mask\n      color = (255, 255, 255, 200)\n      for y in range(y0, y1):\n          for x in range(x0, x1):\n              if mask_array[y - y0, x - x0] > 128:  # Threshold for mask\n                  overlay_draw.point((x, y), fill=color)\n\n      # Save individual mask and its overlay\n      mask_filename = f\"{item['label']}_{i}_mask.png\"\n      overlay_filename = f\"{item['label']}_{i}_overlay.png\"\n\n      mask.save(os.path.join(output_dir, mask_filename))\n\n      # Create and save overlay\n      composite = Image.alpha_composite(im.convert('RGBA'), overlay)\n      composite.save(os.path.join(output_dir, overlay_filename))\n      print(f\"Saved mask and overlay for {item['label']} to {output_dir}\")\n  \n  # Save final result image\n  final_image_path = os.path.join(output_dir, f\"final_result_{items_to_detect}.png\")\n  final_result_image.save(final_image_path)\n  print(f\"\\nSaved final integrated result to {final_image_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:59.341066Z","iopub.execute_input":"2025-11-14T09:51:59.341331Z","iopub.status.idle":"2025-11-14T09:51:59.361154Z","shell.execute_reply.started":"2025-11-14T09:51:59.341312Z","shell.execute_reply":"2025-11-14T09:51:59.359849Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"image_example_3 = \"/kaggle/input/lab2-initial-data/pics/example3.jpg\"\nitems_to_detect = \"Animals\"\nextract_segmentation_masks(image_example_3, items_to_detect, schema=list[Element])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:51:59.362157Z","iopub.execute_input":"2025-11-14T09:51:59.363005Z","iopub.status.idle":"2025-11-14T09:52:08.228566Z","shell.execute_reply.started":"2025-11-14T09:51:59.362967Z","shell.execute_reply":"2025-11-14T09:52:08.227006Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image as IMG\nIMG(filename='segmentation_outputs/final_result_Animals.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:52:08.230076Z","iopub.execute_input":"2025-11-14T09:52:08.230404Z","iopub.status.idle":"2025-11-14T09:52:08.264287Z","shell.execute_reply.started":"2025-11-14T09:52:08.230381Z","shell.execute_reply":"2025-11-14T09:52:08.263112Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"items_to_detect_2 = \"People\"\nextract_segmentation_masks(image_example_3, items_to_detect_2, schema=list[Element])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:52:08.267874Z","iopub.execute_input":"2025-11-14T09:52:08.268137Z","iopub.status.idle":"2025-11-14T09:52:22.749719Z","shell.execute_reply.started":"2025-11-14T09:52:08.268122Z","shell.execute_reply":"2025-11-14T09:52:22.748518Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import Image as IMG\nIMG(filename='segmentation_outputs/final_result_People.png')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:52:22.750266Z","iopub.execute_input":"2025-11-14T09:52:22.750540Z","iopub.status.idle":"2025-11-14T09:52:22.777676Z","shell.execute_reply.started":"2025-11-14T09:52:22.750517Z","shell.execute_reply":"2025-11-14T09:52:22.775602Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_5_1_4_'></a>[**>>> Bonus 2 (Take home):**](#toc0_)\n\nTry asking the model with `another image` of your choosing with `different types of elements`, prompt the model to make the segmentation on one type of element. Try again with a `stronger Gemini model`. **Discuss and compare** the results from both models.","metadata":{}},{"cell_type":"code","source":"# Answer here\n'''\n`gemini-2.5-flash-lite` (The Weaker Model):\nModel couldn't distinguish the oranges, mixing all the fruit on the vendor together.\n\n`gemini-2.5-flash` (The Stronger Model):\nIts segmentation mask (the green area) is highly accurate and precisely outlines the pile of oranges.\n'''\nfrom IPython.display import Image as IMG\nfrom IPython.display import display, Markdown\nimport os\nimport time\n\nmy_image_path = \"/kaggle/input/my-image/street-vendor.jpg\"\nmy_items_to_detect = \"Oranges\"\n\noutput_dir_flash = \"segmentation_outputs_flash\"\nextract_segmentation_masks(my_image_path, my_items_to_detect, schema=list[Element], model_name = \"gemini-2.5-flash-lite\",output_dir = output_dir_flash)\ndisplay(IMG(filename='segmentation_outputs_flash/final_result_Oranges.png'))\n\ntime.sleep(2)\n\noutput_dir_pro = \"segmentation_outputs_pro\"\nextract_segmentation_masks(my_image_path, my_items_to_detect, schema=list[Element], model_name = \"gemini-2.5-flash\" ,output_dir = output_dir_pro)\ndisplay(IMG(filename='segmentation_outputs_pro/final_result_Oranges.png'))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:52:22.779576Z","iopub.execute_input":"2025-11-14T09:52:22.780675Z","iopub.status.idle":"2025-11-14T09:53:00.293067Z","shell.execute_reply.started":"2025-11-14T09:52:22.780628Z","shell.execute_reply":"2025-11-14T09:53:00.291383Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n#### <a id='toc1_2_5_2_'></a>[**1.1.2 Text + Video**](#toc0_)\n\nModern LLMs like Gemini are multimodal, meaning they can analyze **video content** to extract information, generate summaries, and answer specific questions. This capability turns unstructured video into a source of analyzable data, which is highly valuable for data mining tasks.\n\nGemini processes video by analyzing both its **visual track** (by sampling frames at one frame-per-second by default) and its **audio track** simultaneously.\n\nWe are going to explore how to process local video files and YouTube URLs, and how to do timestamp queries and video clipping, although this last one will be discussed in section `10.5 Information Extraction and Grounding`.\n\n**Important Note on Token Usage:** Be aware that video processing is extremely **token-intensive**, consuming hundreds of tokens for every second of video. Use features like clipping and short video clips to manage your usage effectively, especially on the free tier.\n\n[Gemini's Video Understanding Documentation](https://ai.google.dev/gemini-api/docs/video-understanding)","metadata":{}},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_5_2_1_'></a>[**1.1.2.1 Uploading local video:**](#toc0_)\n\n[Upload a video file - documentation](https://ai.google.dev/gemini-api/docs/video-understanding#upload-video)","metadata":{}},{"cell_type":"code","source":"import time\n\n# Function that checks the status of our uploaded file, if it is ready to be used in the prompt\ndef wait_until_file_is_active(client, uploaded_file, timeout=60):\n    wait_time = 0\n    while wait_time < timeout:\n        file_status = client.files.get(name=uploaded_file.name)\n        if file_status.state == \"ACTIVE\":\n            return file_status\n        print(f\"Waiting for file {file_status.name} to become ACTIVE (currently {file_status.state})...\")\n        time.sleep(2)\n        wait_time += 2\n    raise TimeoutError(f\"File {uploaded_file.name} did not become ACTIVE within {timeout} seconds.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:53:00.294435Z","iopub.execute_input":"2025-11-14T09:53:00.294827Z","iopub.status.idle":"2025-11-14T09:53:00.300893Z","shell.execute_reply.started":"2025-11-14T09:53:00.294801Z","shell.execute_reply":"2025-11-14T09:53:00.299737Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\n# The next video was downloaded from this youtube link: https://www.youtube.com/watch?v=oKvVaOvyKPg\n# It's under the Creative Commons License, free to use \nexample_video = client.files.upload(file=\"/kaggle/input/lab2-initial-data/videos/video_example.mp4\")\nwait_until_file_is_active(client, example_video)\n\ninput_prompt = [example_video, \"Make a summary of everything said and happening in the video, and make a transcript verbatim of everything said in it.\"]\ntext_response_video, logs_video = prompt_gemini(input_prompt = input_prompt, with_tokens_info = True)\n\n# We can observe how the input tokens grew bigger in usage\n# By default Gemini uses 1 fps in medium definition for the media, but to save more on token usage we can set 0.1 fps and low definition\n# We explore this in section 10.5 of this notebook\nprint(logs_video)\ndisplay(Markdown(text_response_video))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:53:00.301554Z","iopub.execute_input":"2025-11-14T09:53:00.301748Z","iopub.status.idle":"2025-11-14T09:53:32.935645Z","shell.execute_reply.started":"2025-11-14T09:53:00.301731Z","shell.execute_reply":"2025-11-14T09:53:32.933880Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from pydantic import BaseModel\n\n# Output schema for the video analysis\nclass Transcript(BaseModel):\n    timestamp: str\n    text: str\nclass VideoAnalysis(BaseModel):\n    summary: str\n    transcript: list[Transcript]\n\nstruct_response_video, logs_video_struct = prompt_gemini(input_prompt = input_prompt, schema=VideoAnalysis, with_tokens_info = True)\n\nprint(logs_video_struct)\nprint(struct_response_video)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:53:32.936429Z","iopub.execute_input":"2025-11-14T09:53:32.936736Z","iopub.status.idle":"2025-11-14T09:53:48.069286Z","shell.execute_reply.started":"2025-11-14T09:53:32.936712Z","shell.execute_reply":"2025-11-14T09:53:48.068441Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\nstruct_resp_dict = json.loads(struct_response_video)\ndisplay(Markdown(f\"**Summary:**<br> {struct_resp_dict['summary']}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:53:48.070276Z","iopub.execute_input":"2025-11-14T09:53:48.070573Z","iopub.status.idle":"2025-11-14T09:53:48.076551Z","shell.execute_reply.started":"2025-11-14T09:53:48.070555Z","shell.execute_reply":"2025-11-14T09:53:48.075548Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for timestamp_text in struct_resp_dict[\"transcript\"]:\n    print(f\"{timestamp_text['timestamp']} --- {timestamp_text['text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:53:48.077562Z","iopub.execute_input":"2025-11-14T09:53:48.077830Z","iopub.status.idle":"2025-11-14T09:53:48.103809Z","shell.execute_reply.started":"2025-11-14T09:53:48.077807Z","shell.execute_reply":"2025-11-14T09:53:48.102091Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\n##### <a id='toc1_2_5_2_2_'></a>[**1.1.2.2 Using YouTube video:**](#toc0_)\n\n[Pass YouTube URLs - documentation](https://ai.google.dev/gemini-api/docs/video-understanding#youtube)","metadata":{}},{"cell_type":"code","source":"from google.genai import types\n\n# The video refers to a summary explanation of the anime 'One Piece'\ninput_prompt = [types.Part(file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=rux_f7kctmY')),\n                types.Part(text=\"Make a summary of everything said and happening in the video, and make a transcript verbatim of everything said in it.\")]\nstruct_resp_yt_video, logs_yt_video_struct = prompt_gemini(input_prompt = input_prompt, schema=VideoAnalysis, with_parts = True, with_tokens_info = True)\nstruct_yt_resp_dict = json.loads(struct_resp_yt_video)\nprint(logs_yt_video_struct)\nprint(struct_yt_resp_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:53:48.105014Z","iopub.execute_input":"2025-11-14T09:53:48.105344Z","iopub.status.idle":"2025-11-14T09:54:25.797730Z","shell.execute_reply.started":"2025-11-14T09:53:48.105326Z","shell.execute_reply":"2025-11-14T09:54:25.794946Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(Markdown(f\"**Summary:**<br> {struct_yt_resp_dict['summary']}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:25.798703Z","iopub.execute_input":"2025-11-14T09:54:25.799010Z","iopub.status.idle":"2025-11-14T09:54:25.808659Z","shell.execute_reply.started":"2025-11-14T09:54:25.798982Z","shell.execute_reply":"2025-11-14T09:54:25.806902Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for timestamp_text in struct_yt_resp_dict[\"transcript\"]:\n    print(f\"{timestamp_text['timestamp']} --- {timestamp_text['text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:25.809942Z","iopub.execute_input":"2025-11-14T09:54:25.810267Z","iopub.status.idle":"2025-11-14T09:54:25.838433Z","shell.execute_reply.started":"2025-11-14T09:54:25.810230Z","shell.execute_reply":"2025-11-14T09:54:25.836824Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n#### <a id='toc1_2_5_3_'></a>[**1.1.3 Text + Audio**](#toc0_)\n\nGemini's multimodal capabilities extend to **audio understanding**, allowing it to process sound files to transcribe speech, summarize content, and answer questions. This transforms audio recordings (like interviews, meetings, or customer service calls) from unstructured data into a text-based, analyzable format.\n\nThe process of working with audio in Gemini is very similar to video understanding, but simpler:\n\n*   **Similarities**: You provide audio files using the same two methods: **uploading them** via the File API for larger files or repeated use, or passing them **inline** for smaller, one-off analyses. You can also perform **timestamp-based analysis** (e.g., `02:30`), allowing you to query specific moments in an audio clip just as you would with a video.\n\n*   **Key Difference**: Audio processing is significantly less complex and **less token-intensive** than video. This is because it only involves a single stream of data (sound), whereas video requires processing both a visual track (frames) and an audio track.\n\n**Token Usage:** While more efficient than video, audio still consumes approximately **32 tokens per second**. The total length of all audio files in a single prompt cannot exceed 9.5 hours.\n\n[Gemini's Audio Understanding Documentation](https://ai.google.dev/gemini-api/docs/audio)","metadata":{}},{"cell_type":"markdown","source":"For the following example we are using the audio extracted from Prof. Chen's Data Mining Lecture: [2025-ISA5810 Session 1 - Introduction and Data Part I](https://www.youtube.com/watch?v=VWnN4J7Zblw)","metadata":{}},{"cell_type":"code","source":"from pydantic import BaseModel\n\n# Schema for the output of our audio analysis\nclass Transcript(BaseModel):\n    timestamp: str\n    text: str\nclass AudioAnalysis(BaseModel):\n    explanation: str\n    segment_transcript: list[Transcript]\n    complete_transcript: list[Transcript]\n\nexample_audio = client.files.upload(file=\"/kaggle/input/lab2-initial-data/audios/audio_example.mp3\")\nwait_until_file_is_active(client, example_audio)\n\nstart_segment = \"00:46\"\nend_segment = \"00:56\"\ntask = \\\n        f\"\"\"\n        Based on an audio from a lecture of data mining, provide the following:\n        Enrich and give more context explaining the concepts discussed in the lecture from the segment: {start_segment} to {end_segment}.\n        The complete transcript and a version of it only for the specified segment.\n        \"\"\"\ninput_prompt = [example_audio, task]\nstruct_response_audio, logs_audio_struct = prompt_gemini(input_prompt = input_prompt, schema=AudioAnalysis, with_tokens_info = True)\n\n\nstruct_audio_resp_dict = json.loads(struct_response_audio)\nprint(logs_audio_struct)\nprint(struct_audio_resp_dict)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:25.843119Z","iopub.execute_input":"2025-11-14T09:54:25.843594Z","iopub.status.idle":"2025-11-14T09:54:37.283779Z","shell.execute_reply.started":"2025-11-14T09:54:25.843569Z","shell.execute_reply":"2025-11-14T09:54:37.282624Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"display(Markdown(f\"**Explanation:**<br> {struct_audio_resp_dict['explanation']}\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:37.284555Z","iopub.execute_input":"2025-11-14T09:54:37.284799Z","iopub.status.idle":"2025-11-14T09:54:37.291833Z","shell.execute_reply.started":"2025-11-14T09:54:37.284781Z","shell.execute_reply":"2025-11-14T09:54:37.290168Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for timestamp_text in struct_audio_resp_dict[\"segment_transcript\"]:\n    print(f\"{timestamp_text['timestamp']} --- {timestamp_text['text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:37.292792Z","iopub.execute_input":"2025-11-14T09:54:37.293135Z","iopub.status.idle":"2025-11-14T09:54:37.317042Z","shell.execute_reply.started":"2025-11-14T09:54:37.293091Z","shell.execute_reply":"2025-11-14T09:54:37.315172Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for timestamp_text in struct_audio_resp_dict[\"complete_transcript\"]:\n    print(f\"{timestamp_text['timestamp']} --- {timestamp_text['text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:37.317957Z","iopub.execute_input":"2025-11-14T09:54:37.318211Z","iopub.status.idle":"2025-11-14T09:54:37.338545Z","shell.execute_reply.started":"2025-11-14T09:54:37.318193Z","shell.execute_reply":"2025-11-14T09:54:37.337859Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_5_3_1_'></a>[**>>> Bonus 3 (Take home):**](#toc0_)\n\nUse the example of the prompt in `section 1.1.3 Text + Audio`, but change it to use video as the input, the same way as in `section 1.1.2 Text + Video`. Use the video that is inside the directory `videos/video_lecture_example.mp4` in this lab. The `video` corresponds to the `original lecture segment` where the `audio example` was extracted. Prompt it using the `same time segment as in the example`, but you need to add in the schema a new extraction item called `video_insight`, the model needs to provide an explanation using what is being shown in the screen during that time segment.","metadata":{}},{"cell_type":"code","source":"# Answer here\n\nfrom pydantic import BaseModel\nfrom IPython.display import display, Markdown\nimport json\n\nclass Transcript(BaseModel):\n    timestamp: str\n    text: str\nclass VideoAnalysis(BaseModel):\n    explanation: str\n    video_insight: str\n    segment_transcript: list[Transcript]\n    complete_transcript: list[Transcript]\n\n\nlecture_example_video = client.files.upload(file=\"/kaggle/input/lab2-initial-data/videos/video_lecture_example.mp4\")\nwait_until_file_is_active(client, lecture_example_video)\n\nstart_segment = \"00:46\"\nend_segment = \"00:56\"\n\ntask = \\\n        f\"\"\"\n        Based on an video from a lecture of data mining, provide the following:\n        Provide a new 'video_insight' explanation using what is being shown on the screen during that time segment.\n        Enrich and give more context explaining the concepts discussed in the lecture from the segment: {start_segment} to {end_segment}.\n        The complete transcript and a version of it only for the specified segment.\n        \"\"\"\ninput_prompt_b3 = [lecture_example_video,task]\ntext_response_video_b3, logs_video_b3 = prompt_gemini(input_prompt = input_prompt_b3, schema = VideoAnalysis, with_tokens_info = True)\n\nstruct_video_resp_dict_b3 = json.loads(text_response_video_b3)\n\ndisplay(Markdown(f\"**Explanation:**<br> {struct_video_resp_dict_b3['explanation']}\"))\n\ndisplay(Markdown(f\"**Video Insight:**<br> {struct_video_resp_dict_b3['video_insight']}\"))\n\nfor timestamp_text in struct_video_resp_dict_b3[\"segment_transcript\"]:\n    print(f\"{timestamp_text['timestamp']} --- {timestamp_text['text']}\\n\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:54:37.339436Z","iopub.execute_input":"2025-11-14T09:54:37.339757Z","iopub.status.idle":"2025-11-14T09:55:04.546073Z","shell.execute_reply.started":"2025-11-14T09:54:37.339735Z","shell.execute_reply":"2025-11-14T09:55:04.544881Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n### <a id='toc1_2_6_'></a>[**1.2 Tool Calling:**](#toc0_)\n\nUp to now we have seen how prompting, text and multimodal, lets a model **generate responses directly.** But sometimes we want the model to do more than just talk: to **fetch information, compute something, or take an action.** This is where tool calling (also called *function calling* or *agents*) comes in.\n\n#### <a id='toc1_2_6_1_'></a>[General Idea](#toc0_)\n\nTool calling means the model can decide, instead of replying only with text, to **call an external function** that you have made available. You give the model a list of tools, each described by their **name, purpose, and input parameters,** and the model chooses when to use them.\n\nSo the order is something like this:\n1.\tYou declare tools (name, purpose, inputs).\n2.\tThe model checks the user request and may return a function_call.\n3.\tYour system runs the function.\n4.\tThe result is sent back, and the model uses it to craft the final response.\n\nGemini supports this pattern with extra features:\n\n\t**Modes:** AUTO (default), ANY (always tool), NONE (no tool).\n\n\t**Automatic handling:** the SDK can manage declarations + execution for you.\n\n\t**Advanced use:** parallel calls, sequential chaining, and thinking with thought signatures to keep context.\n\n\t**Documentation:** [Gemini's Function Calling](https://ai.google.dev/gemini-api/docs/function-calling?example=meeting) \n\nTool calling turns Gemini into an agent that can:\n\n\t`Access fresh data` (e.g. APIs, databases).\n\n\t`Extend abilities` (e.g. calculators, charts, file readers).\n\n\t`Take actions` (e.g. emails, devices, apps).\n","metadata":{}},{"cell_type":"code","source":"def simple_math(a: int, b: int, operation: str) -> int | float:\n    \"\"\"\n    Performs a simple mathematical operation on two numbers.\n    Args:\n        a: The first number.\n        b: The second number.\n        operation: The operation to perform. Must be one of 'add', 'subtract', 'multiply', or 'divide'.\n    Returns:\n        The result of the mathematical operation.\n    \"\"\"\n    # Double validating the input\n    a = int(a)\n    b = int(b)\n    if operation == 'add':\n        return a + b\n    elif operation == 'subtract':\n        return a - b\n    elif operation == 'multiply':\n        return a * b\n    elif operation == 'divide':\n        return a / b\n    else:\n        raise ValueError(\"Invalid operation. Please choose from 'add', 'subtract', 'multiply', or 'divide'.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:04.548089Z","iopub.execute_input":"2025-11-14T09:55:04.549380Z","iopub.status.idle":"2025-11-14T09:55:04.557154Z","shell.execute_reply.started":"2025-11-14T09:55:04.549319Z","shell.execute_reply":"2025-11-14T09:55:04.555497Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"simple_math_declaration = {\n        'name': 'simple_math',\n        'description': 'Perform a simple mathematical operation',\n        'parameters': {\n          'type': 'object',\n          'properties': {\n            'a': {\n              'type': 'integer',\n              'description': 'The first number',\n            },\n            'b': {\n              'type': 'integer',\n              'description': 'The second number',\n            },\n            'operation': {\n                'type': 'string',\n                'enum': [\"add\", \"subtract\", \"multiply\", \"divide\"],\n                'description': \"The mathematical operation to perform, one of 'add', 'subtract', 'multiply', 'divide'\",\n            }\n          },\n          'required': ['a', 'b', 'operation'],\n        },\n    }","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:04.558475Z","iopub.execute_input":"2025-11-14T09:55:04.558762Z","iopub.status.idle":"2025-11-14T09:55:04.590562Z","shell.execute_reply.started":"2025-11-14T09:55:04.558737Z","shell.execute_reply":"2025-11-14T09:55:04.588940Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# IMPORTANT: For more functions you can add new ones in this dictionary \navailable_functions = {\n  'simple_math': simple_math,\n  # HINT: For the bonus question here is where you add the second function\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:04.591484Z","iopub.execute_input":"2025-11-14T09:55:04.591838Z","iopub.status.idle":"2025-11-14T09:55:04.615064Z","shell.execute_reply.started":"2025-11-14T09:55:04.591819Z","shell.execute_reply":"2025-11-14T09:55:04.614330Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plotting all four emotions for text length\nprint(\"\\nTesting the function\")\nresult = simple_math(1, 2, \"add\")\nprint(f\"1 + 2 = {result}\")\nresult = simple_math(1, 2, \"subtract\")\nprint(f\"1 - 2 = {result}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:04.616072Z","iopub.execute_input":"2025-11-14T09:55:04.616311Z","iopub.status.idle":"2025-11-14T09:55:04.644314Z","shell.execute_reply.started":"2025-11-14T09:55:04.616291Z","shell.execute_reply":"2025-11-14T09:55:04.642560Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from google.genai import types\n\n# Here is where we configure our tools \ntools = types.Tool(function_declarations=[simple_math_declaration])\nnew_config = types.GenerateContentConfig(tools=[tools])\nsystem_prompt = \"You are a helpful tool assistant. You know how to infer what the user wants and translate into function parameters for a tool.\"\n\nuser_prompt_text = [\n    types.Content(\n        role=\"user\", parts=[types.Part(text=\"What is 5 plus 10? also what is 10 multiplied by 3?\")]\n    )\n]\n\n# We passed down the new config for Gemini to understand our tool\nresponse_tool_calling = prompt_gemini(input_prompt = user_prompt_text, system_instruction=system_prompt, new_config=new_config, temperature=1.0, with_tools=True)\nprint(f\"response: {response_tool_calling}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:04.645811Z","iopub.execute_input":"2025-11-14T09:55:04.646482Z","iopub.status.idle":"2025-11-14T09:55:05.560101Z","shell.execute_reply.started":"2025-11-14T09:55:04.646458Z","shell.execute_reply":"2025-11-14T09:55:05.557127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Extract tool call details, it may not be in the first part\ntool_call = response_tool_calling.candidates[0].content.parts[0].function_call\ntool_call","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:05.562151Z","iopub.execute_input":"2025-11-14T09:55:05.562548Z","iopub.status.idle":"2025-11-14T09:55:05.571299Z","shell.execute_reply.started":"2025-11-14T09:55:05.562520Z","shell.execute_reply":"2025-11-14T09:55:05.569568Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"tool_call.args","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:05.572561Z","iopub.execute_input":"2025-11-14T09:55:05.572855Z","iopub.status.idle":"2025-11-14T09:55:05.593186Z","shell.execute_reply.started":"2025-11-14T09:55:05.572834Z","shell.execute_reply":"2025-11-14T09:55:05.591661Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"function_output = None\noperations = []\nresults = []\nfunctions = []\nif tool_call:\n  # There may be multiple tool calls in the response\n  for part in response_tool_calling.candidates[0].content.parts:\n    if part.function_call:\n        tool = part.function_call\n        # Ensure the function is available, and then call it\n        if function_to_call := available_functions.get(tool.name):\n            print('Calling function:', tool.name)\n            print('Arguments:', tool.args)\n            functions.append(tool.name)\n            operations.append(tool.args[\"operation\"])\n            function_output = function_to_call(**tool.args)\n            results.append(function_output)\n            print('Function output:', function_output)\n        else:\n            print('Function', tool.name, 'not found')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:05.594870Z","iopub.execute_input":"2025-11-14T09:55:05.595894Z","iopub.status.idle":"2025-11-14T09:55:05.616815Z","shell.execute_reply.started":"2025-11-14T09:55:05.595848Z","shell.execute_reply":"2025-11-14T09:55:05.615787Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Markdown\n\nintermediate_responses = []\nresults_to_discuss = []\nfor operation, result in zip(operations, results):\n    results_to_discuss.append(f\"The arithmetic operation was: {operation} ; and the result was: {result}\")\n\nfor result in results_to_discuss:\n    input_prompt_analysis = [f\"Get insights from the tool call results just objective short analysis: {result}\"]\n    operation_insights, _ = prompt_gemini(input_prompt = input_prompt_analysis, with_tokens_info = True)\n    intermediate_responses.append(operation_insights)\n    display(Markdown(operation_insights))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:05.618082Z","iopub.execute_input":"2025-11-14T09:55:05.618708Z","iopub.status.idle":"2025-11-14T09:55:07.272126Z","shell.execute_reply.started":"2025-11-14T09:55:05.618681Z","shell.execute_reply":"2025-11-14T09:55:07.270921Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from IPython.display import display, Markdown\nfunction_response_parts = []\nfor tool_name, response_insights in zip(functions, intermediate_responses):\n    function_response_part = types.Part.from_function_response(\n        name=tool_name,\n        response={\"insights\": response_insights},\n        )\n    function_response_parts.append(function_response_part)\n\n# This is the full conversation history for the final step\nconversation_history = [user_prompt_text[0],\n                     response_tool_calling.candidates[0].content,\n                     types.Content(role=\"user\", parts=function_response_parts)]\n# print(conversation_history)\n\nsystem_prompt_2 = \"You are a helpful concluding assistant. The user asked a question, a tool was used to do an arithmetic operation, \" \\\n\"and you have been given the analysis. Provide a final, conclusive answer to the user based on these insights, explained in a proper way\"\n\nfinal_response = prompt_gemini(input_prompt = conversation_history, system_instruction=system_prompt_2, new_config=new_config, with_tools=True)\n# final_response.text\ndisplay(Markdown(final_response.text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:07.273188Z","iopub.execute_input":"2025-11-14T09:55:07.274121Z","iopub.status.idle":"2025-11-14T09:55:08.192331Z","shell.execute_reply.started":"2025-11-14T09:55:07.274079Z","shell.execute_reply":"2025-11-14T09:55:08.191445Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_6_1_1_'></a>[**>>> Bonus 4 (Take home):**](#toc0_)\n\n`Create two new functions` to be passed down as tools to Gemini at the same time, **it is fine if they are simple functions** (e.g. reverse order of a string text characters).\n\n`Create one prompt` where you ask the model to use both functions. You can see our example where we ask the model to do two operations but this time it needs to call two different functions instead of just one with different parameters.\n\nShow the following:\n1. The output of how Gemini choose to call the two different functions.\n2. The output of both functions after passing down the parameters.\n3. The final response by the model after passing down the whole execution (by function call like in our example).\n\n**What we want to see is the model choosing correctly what tool to use based on your prompt.**\n\n**NOTE:** Check the `HINT` labeled notebook cell to see where to add the extra function in this pipeline.","metadata":{}},{"cell_type":"code","source":"# Answer here\nfrom google.genai import types\nfrom IPython.display import display, Markdown\nfrom IPython.display import display, Markdown\n\ndef reverse_string(text: str) -> str:\n    \"\"\"\n    Reverses the order of characters in a given text string\n    Returns:\n        The reversed string\n    \"\"\"\n    return text[::-1]\n\ndef count_characters(text: str) -> int:\n    \"\"\"\n    Counts the number of characters in a given text string\n    Returns:\n        The number of characters\n    \"\"\"\n    return len(text)\n\nreverse_string_declaration = {\n        'name': 'reverse_string',\n        'description': 'Reverses the order of characters in a given text string',\n        'parameters': {\n          'type': 'object',\n          'properties': {\n            'text': {\n              'type': 'string',\n              'description': 'The text to reverse',\n            },\n          },\n          'required': ['text'],\n        },\n    }\n\ncount_characters_declaration = {\n        'name': 'count_characters',\n        'description': 'Counts the number of characters in a given text string',\n        'parameters': {\n          'type': 'object',\n          'properties': {\n            'text': {\n              'type': 'string',\n              'description': 'The text to count',\n            },\n          },\n          'required': ['text'],\n        },\n    }\n\navailable_functions = {\n  'simple_math': simple_math,\n  'reverse_string': reverse_string,\n    'count_characters': count_characters\n}\n\nmy_tools = types.Tool(function_declarations=[simple_math_declaration,reverse_string_declaration,count_characters_declaration])\nmy_new_config = types.GenerateContentConfig(tools=[my_tools])\nsystem_prompt = \"You are a helpful tool assistant. You know how to infer what the user wants and translate into function parameters for a tool.\"\n\nmy_user_prompt_text = [\n    types.Content(\n        role=\"user\", parts=[types.Part(text=\"What does 'banana' reverse version look like? How many characters in the text?\")]\n    )\n]\n\nprint(\"-----how Gemini choose to call the two different functions-----\")\nmy_response_tool_calling = prompt_gemini(input_prompt = my_user_prompt_text, system_instruction=system_prompt, new_config=my_new_config, temperature=1.0, with_tools=True)\n\nfunction_calls_to_execute = []\nfor part in my_response_tool_calling.candidates[0].content.parts:\n    if part.function_call:\n        print(part.function_call) \n        function_calls_to_execute.append(part)\n\nprint(\"\\n-----both functions after passing down the parameters-----\")\n\nmy_function_output = None\nfunctions = []\nresults_for_model = []\nfor part in function_calls_to_execute:\n    tool = part.function_call\n    if function_to_call := available_functions.get(tool.name):\n            print('Calling function:', tool.name)\n            print('Arguments:', tool.args)\n            my_function_output = function_to_call(**tool.args)\n            results_for_model.append(\n            types.Part.from_function_response(\n                name=tool.name, \n                response={\"result\": my_function_output})\n            )\n            print('Function output:', my_function_output)\n    else:\n            print('Function', tool.name, 'not found')\n\nprint(\"\\n----- final response by the model after passing down the whole execution-----\")\n\nmy_conversation_history = [my_user_prompt_text[0],\n                     my_response_tool_calling.candidates[0].content,\n                     types.Content(role=\"user\", parts=results_for_model)]\n\nmy_system_prompt = \"You are a helpful concluding assistant. The user asked a question, a tool was used to to get the answers. \" \\\n\"Provide a final, conclusive answer to the user based on these tool results, explained in a proper way\"\n\nmy_final_response = prompt_gemini(input_prompt = my_conversation_history, system_instruction=my_system_prompt, new_config=my_new_config, with_tools=True)\n\ndisplay(Markdown(my_final_response.text))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:08.193241Z","iopub.execute_input":"2025-11-14T09:55:08.193938Z","iopub.status.idle":"2025-11-14T09:55:09.275827Z","shell.execute_reply.started":"2025-11-14T09:55:08.193892Z","shell.execute_reply":"2025-11-14T09:55:09.272884Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n### <a id='toc1_2_7_'></a>[**1.3 Information Extraction and Grounding:** - Expanded explanation](#toc0_)\n\nWe've already seen how to get structured data from LLMs using techniques like structured prompting and function calling. These methods are powerful for turning an LLM's understanding into a machine-readable format. However, as we build more complex applications, two critical challenges emerge: **trust** and **scalability**.\n\n1.  **Trust and Verifiability (Grounding):** How can we be sure the information an LLM provides is accurate and actually comes from the source text we gave it? LLMs can sometimes \"hallucinate\" or infer information that isn't present. **Grounding** is the process of linking every piece of extracted data directly back to its specific origin in the source document. This creates a verifiable audit trail, allowing a human or another system to confirm the information's source, which is essential in high-stakes domains like medicine, finance, and legal analysis.\n\n2.  **Scalability and Reliability (Extraction):** While function calling is excellent for specific tasks, building a robust pipeline to extract complex information from long, messy documents requires a more specialized approach. We need a system that can handle large texts, consistently enforce a desired output schema across thousands of documents, and make the results easy for humans to review.\n\nThis is where specialized libraries for information extraction come in, providing a framework that prioritizes both grounding and reliable structuring of data.","metadata":{}},{"cell_type":"markdown","source":"---\n#### <a id='toc1_2_7_1_'></a>[**`langextract`: A Library for Grounded Extraction**](#toc0_)\n\nGoogle's open-source Python library, `langextract`, is designed specifically to solve these problems. It uses the power of LLMs to convert unstructured text into structured data, with a strong emphasis on reliability and traceability. Instead of just getting a JSON object, `langextract` provides a complete solution for building trustworthy extraction pipelines.\n\nHere are its key features:\n\n*   **Precise Source Grounding:** This is the library's core strength. Every single piece of extracted data is mapped to its exact character position in the original text. This means you can always trace an output back to its source, building trust and allowing for easy verification.\n*   **Reliable Structured Outputs:** We define the structure of our desired output by providing a few examples (few-shot prompting). `langextract` uses these examples to guide the LLM, ensuring the output consistently follows our defined schema. This makes the data clean, predictable, and ready for analysis or storage.\n*   **Adaptable to Any Domain without Fine-Tuning:** We don't need to collect a large dataset or retrain a model. By simply changing our natural language instructions and examples, you can adapt `langextract` to new domains, whether it's extracting clinical data from medical notes, key clauses from legal contracts, or financial figures from earnings reports.\n*   **Optimized for Long Documents:** The library is built to handle lengthy texts that might exceed a standard LLM's context window. It employs smart chunking and multi-pass strategies to effectively find \"needles in a haystack\" within large documents.\n*   **Flexible LLM Support:** `langextract` is model-agnostic. While it's optimized for powerful models like Google's Gemini or OpenAI models, it also supports local, open-source models (via Ollama), or even custom providers, giving us the flexibility to balance performance, cost, and data privacy.\n\n**`Github repository:`** [langextract](https://github.com/google/langextract)","metadata":{}},{"cell_type":"markdown","source":"---\nHere we define our main function to call for langextract information extraction, note that there are some constants in the functions that we are not going to change for the example but it would be required to explore and understand in the exercise. In this function we obtain the resulting raw extracted information into a .jsonl file and the visualization into a .html file. Check the documentation for more information.\n\nThe files will be saved in the following directory: `results/info_extractions`","metadata":{}},{"cell_type":"code","source":"import os\nimport langextract as lx\n\n# ->  -> \n\n# We define our main langextract function \ndef grounded_info_extraction(input_documents, prompt, examples, file_name, model_id =\"gemini-2.5-flash-lite\", extraction_passes = 1, max_workers = 5, max_char_buffer = 2000):\n    result = lx.extract(\n        text_or_documents=input_documents,\n        prompt_description=prompt,\n        examples=examples,\n        model_id=model_id,\n        extraction_passes=extraction_passes,    # Improves recall through multiple passes over the same text, needs temperature above 0.0\n        max_workers=max_workers,         # Parallel processing for speed, remember there are API call rate limits, so do not abuse\n        max_char_buffer=max_char_buffer    # Smaller contexts for better accuracy, currently: 1000 characters per batch\n    )\n\n    # Display results\n    print(f\"Extracted {len(result.extractions)} entities:\\n\")\n    for extraction in result.extractions:\n        print(f\" {extraction.extraction_class}: '{extraction.extraction_text}'\")\n        if extraction.attributes:\n            for key, value in extraction.attributes.items():\n                print(f\"  - {key}: {value}\")\n    \n    output_dir = \"./results/info_extractions\"\n    os.makedirs(output_dir, exist_ok=True)\n    # Save results to JSONL\n    lx.io.save_annotated_documents([result], output_name=f\"{file_name}.jsonl\", output_dir=output_dir)\n\n    # Generate interactive visualization\n    html_content = lx.visualize(f\"{output_dir}/{file_name}.jsonl\")\n    with open(f\"{output_dir}/{file_name}_vis.html\", \"w\") as f:\n        if hasattr(html_content, 'data'):\n            f.write(html_content.data)\n        else:\n            f.write(html_content)\n\n    print(f\" Visualization saved to {output_dir}/{file_name}_vis.html\")\n    \n    # returning html content for display\n    return html_content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:09.281490Z","iopub.execute_input":"2025-11-14T09:55:09.281820Z","iopub.status.idle":"2025-11-14T09:55:09.291315Z","shell.execute_reply.started":"2025-11-14T09:55:09.281800Z","shell.execute_reply":"2025-11-14T09:55:09.290082Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_7_1_1_'></a>[**1.3.1 Using a video from youtube:**](#toc0_)\n\nNow let's try with a video lecture from the class in youtube to see how to extract meaningful grounded information from it that could help you study, not only for this class but for any occasion where you need to watch videos to learn.\n\nThe video used is the same Data Mining lecture as the one in section 10.3.3 audio example and in exercise 13's video: [2025-ISA5810 Session 1 - Introduction and Data Part I](https://www.youtube.com/watch?v=VWnN4J7Zblw)","metadata":{}},{"cell_type":"code","source":"#\nfrom google.genai import types\nfrom pydantic import BaseModel, Field\nfrom typing import Literal, Optional, List\n\n# Schema for the lecture analysis\n# A single block of content, which can either be a slide or a transcript segment\nclass ContentBlock(BaseModel):\n    content_type: Literal[\"slide_description\", \"transcript_segment\"]\n    text: str\n    timestamp: Optional[str] = Field(None, description=\"Timestamp is only for 'transcript_segment'\") # This comment will be passed down to the LLM for extra explanation\n\n# The top-level structure for the entire video analysis\nclass LectureAnalysis(BaseModel):\n    title: str\n    video_segment: str\n    content_flow: List[ContentBlock]\n\n    \n# We set the media resolution to low to save tokens' usage, and we use the new schema, other configurations are just like the ones set in the beginning of the section\nnew_config = types.GenerateContentConfig(media_resolution=types.MediaResolution.MEDIA_RESOLUTION_LOW,\n                                         temperature=0.0,\n                                         system_instruction=\"You are a Lecture Analysis assistant. Structure your output in the provided JSON Schema.\",\n                                         max_output_tokens=65535,\n                                         response_modalities=[\"TEXT\"],\n                                         response_mime_type=\"application/json\",\n                                         response_schema=LectureAnalysis,\n                                         safety_settings=SAFETY_SETTINGS)\n\n# We specify 1 frame every 10 seconds for the video because the slides do not change that often, to save tokens\n# Frame rate can be specified from (0.0, 24.0]\n# We also specify the part of the video to clip for the prompt, the specific part is talking about the types of data from 1:50:59 to 1:54:59 so we convert it to seconds\ninput_prompt = [types.Part(file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=VWnN4J7Zblw'),\n                           video_metadata=types.VideoMetadata(fps=0.1, start_offset=\"6658s\", end_offset=\"6899s\")),\n                types.Part(text=\"Provide the transcript verbatim of everything explained in the video lecture, describing the presentation slides' content as well.\")]\n\n# You can try to use better models for this task, just be aware of the API rate and token usage limits\ntranscript_lecture_yt_vid, logs_lecture_yt_vid = prompt_gemini(input_prompt = input_prompt, model_name=\"gemini-2.5-flash-lite\", new_config=new_config, with_parts = True, with_tokens_info = True)\nprint(logs_lecture_yt_vid)\nprint(transcript_lecture_yt_vid)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:09.292182Z","iopub.execute_input":"2025-11-14T09:55:09.292451Z","iopub.status.idle":"2025-11-14T09:55:37.058000Z","shell.execute_reply.started":"2025-11-14T09:55:09.292433Z","shell.execute_reply":"2025-11-14T09:55:37.056128Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#json\nimport json\n\ndef create_langextract_input_from_json(json_filepath: str, output_txt_filepath: str):\n    \"\"\"\n    Reads a structured lecture analysis JSON and converts it into a single,\n    formatted text file suitable for information extraction with langextract.\n    \"\"\"\n    try:\n        with open(json_filepath, 'r') as f:\n            data = json.load(f)\n    except FileNotFoundError:\n        print(f\"Error: The file {json_filepath} was not found.\")\n        return\n\n    concatenated_text = []\n    \n    # Add a header with metadata from the JSON\n    concatenated_text.append(f\"# LECTURE ANALYSIS: {data.get('title', 'Untitled')}\")\n    concatenated_text.append(f\"## Video Segment: {data.get('video_segment', 'Unknown')}\\n\")\n\n    # Process each content block\n    for block in data.get(\"content_flow\", []):\n        content_type = block.get(\"content_type\")\n        text = block.get(\"text\")\n        \n        if content_type == \"slide_description\":\n            concatenated_text.append(f\"[SLIDE DESCRIPTION]\\n{text}\\n\")\n        elif content_type == \"transcript_segment\":\n            timestamp = block.get(\"timestamp\", \"00:00:00\")\n            concatenated_text.append(f\"[TRANSCRIPT - {timestamp}]\\n{text}\\n\")\n            \n    # Join all parts into a single string\n    final_text = \"\\n\".join(concatenated_text)\n\n    # Save to the output file\n    with open(output_txt_filepath, 'w') as f:\n        f.write(final_text)\n        \n    print(f\"Successfully created '{output_txt_filepath}'\")\n    return final_text","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:37.059126Z","iopub.execute_input":"2025-11-14T09:55:37.059456Z","iopub.status.idle":"2025-11-14T09:55:37.068970Z","shell.execute_reply.started":"2025-11-14T09:55:37.059425Z","shell.execute_reply":"2025-11-14T09:55:37.067736Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\nimport json\nimport os\nos.makedirs(\"./results/info_extractions\", exist_ok=True)\n# We save our Gemini output with the lecture analysis in structured format\njson_filepath = \"./results/info_extractions/lecture_analysis.json\" \nstruct_yt_lecture_dict = json.loads(transcript_lecture_yt_vid)\nwith open(json_filepath, \"w\") as f:\n    json.dump(struct_yt_lecture_dict, f, indent=4)\n\n# Let's convert it into a single text for langextract analysis\nlecture_txt = create_langextract_input_from_json(json_filepath=json_filepath, output_txt_filepath=\"./results/info_extractions/lecture_for_langextract.txt\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:37.070228Z","iopub.execute_input":"2025-11-14T09:55:37.070552Z","iopub.status.idle":"2025-11-14T09:55:37.104609Z","shell.execute_reply.started":"2025-11-14T09:55:37.070528Z","shell.execute_reply":"2025-11-14T09:55:37.102944Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#\nimport langextract as lx\nimport textwrap\n\n# Defining the extraction prompt for \"lecture transcript\" data\nprompt_lect = textwrap.dedent(\"\"\"\\\n    Extract key computer science concepts and practical insights from this lecture transcript.\n    Important: Use exact text verbatim from the input for the 'extraction_text' field. Do not paraphrase.\n    Extract entities in order of their appearance in the text, ensuring there are no overlapping text spans.\n\n    Use the 'key_concept' class for core definitions, terms, and fundamental ideas.\n    - 'concept_name': The formal name of the concept (e.g., \"Big O Notation\").\n    - 'definition': The definition provided in the text.\n    - 'difficulty_rating': An inferred difficulty for a beginner (e.g., \"Beginner\", \"Intermediate\", \"Advanced\").\n    - 'simplified_explanation': A concise, generated explanation of the concept for easier understanding.\n\n    Use the 'practical_insight' class for actionable advice, common mistakes, or statements about why a concept is important.\n    - 'insight_type': The category of the advice (e.g., \"Common Pitfall\", \"Best Practice\", \"Critical Importance\").\n    - 'related_concept': The concept this insight applies to.\n    - 'summary_of_insight': The core piece of advice, summarized.\n    \"\"\")\n\n# Providing high-quality examples for the extraction\nexamples_lect = [\n    # Example 1: A lecture snippet on Big O Notation, containing both a concept and a pitfall\n    lx.data.ExampleData(\n        text=\"Okay, let's talk about Big O Notation. At its core, it's a way to describe the complexity \"\n             \"or performance of an algorithm as the input size grows. A common mistake for beginners \"\n             \"is to over-focus on constants. Remember, Big O is about the long-term, asymptotic behavior, \"\n             \"so for an O(n^2 + n) algorithm, you can safely ignore the '+ n' part for analysis.\",\n        extractions=[\n            lx.data.Extraction(\n                extraction_class=\"key_concept\",\n                extraction_text=\"Big O Notation. At its core, it's a way to describe the complexity \"\n                                \"or performance of an algorithm as the input size grows.\",\n                attributes={\n                    \"concept_name\": \"Big O Notation\",\n                    \"definition\": \"A way to describe the complexity or performance of an algorithm as the input size grows.\",\n                    \"difficulty_rating\": \"Beginner\",\n                    \"simplified_explanation\": \"Tells you how an algorithm's runtime or memory usage scales with more data. For example, O(n) means performance grows linearly with the input size n.\"\n                }\n            ),\n            lx.data.Extraction(\n                extraction_class=\"practical_insight\",\n                extraction_text=\"A common mistake for beginners is to over-focus on constants. Remember, \"\n                                \"Big O is about the long-term, asymptotic behavior, so for an O(n^2 + n) \"\n                                \"algorithm, you can safely ignore the '+ n' part for analysis.\",\n                attributes={\n                    \"insight_type\": \"Common Pitfall\",\n                    \"related_concept\": \"Big O Notation\",\n                    \"summary_of_insight\": \"When analyzing Big O, ignore constants and lower-order terms to focus on the dominant growth factor.\"\n                }\n            ),\n        ]\n    ),\n    # Example 2: A lecture snippet comparing sorting algorithms, showing a concept and a best practice\n    lx.data.ExampleData(\n        text=\"Now consider Merge Sort. It's a classic divide-and-conquer algorithm with a time \"\n             \"complexity of O(n log n). This is critically important: while something like Bubble Sort \"\n             \"might be easier to code, its O(n^2) performance makes it totally unsuitable for large datasets. \"\n             \"Always consider your data scale before picking an algorithm.\",\n        extractions=[\n            lx.data.Extraction(\n                extraction_class=\"key_concept\",\n                extraction_text=\"Merge Sort. It's a classic divide-and-conquer algorithm with a time \"\n                                \"complexity of O(n log n).\",\n                attributes={\n                    \"concept_name\": \"Merge Sort\",\n                    \"definition\": \"A divide-and-conquer algorithm with O(n log n) time complexity.\",\n                    \"difficulty_rating\": \"Intermediate\",\n                    \"simplified_explanation\": \"Merge Sort works by repeatedly splitting the input list in half, sorting the halves, and then merging them back together in sorted order.\"\n                }\n            ),\n            lx.data.Extraction(\n                extraction_class=\"practical_insight\",\n                extraction_text=\"while something like Bubble Sort might be easier to code, its O(n^2) performance \"\n                                \"makes it totally unsuitable for large datasets. Always consider your data scale \"\n                                \"before picking an algorithm.\",\n                attributes={\n                    \"insight_type\": \"Best Practice\",\n                    \"related_concept\": \"Algorithm Selection\",\n                    \"summary_of_insight\": \"Choose algorithms based on their performance with large datasets (e.g., O(n log n) over O(n^2)), not just ease of implementation.\"\n                }\n            )\n        ]\n    )\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:37.106254Z","iopub.execute_input":"2025-11-14T09:55:37.106639Z","iopub.status.idle":"2025-11-14T09:55:37.133878Z","shell.execute_reply.started":"2025-11-14T09:55:37.106614Z","shell.execute_reply":"2025-11-14T09:55:37.132734Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"html_content = grounded_info_extraction(lecture_txt, prompt_lect, examples_lect, \"lecture_extraction_example\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:37.134780Z","iopub.execute_input":"2025-11-14T09:55:37.135052Z","iopub.status.idle":"2025-11-14T09:55:41.401954Z","shell.execute_reply.started":"2025-11-14T09:55:37.135027Z","shell.execute_reply":"2025-11-14T09:55:41.400466Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"html_content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:55:41.403228Z","iopub.execute_input":"2025-11-14T09:55:41.403596Z","iopub.status.idle":"2025-11-14T09:55:41.410044Z","shell.execute_reply.started":"2025-11-14T09:55:41.403572Z","shell.execute_reply":"2025-11-14T09:55:41.408916Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n##### <a id='toc1_2_7_1_2_'></a>[**>>> Bonus 5 (Take home):**](#toc0_)\n\nRepeat the steps for information extraction using the same lecture video with some changes:\n1. Pick a different segment of the video where professor is explaining a different topic, we **suggest a 3 to 5 minutes segment**.\n2. Extract the same information again with Gemini and turn it into a single text for input into langextract.\n3. Add a new `extraction_class` with different attributes, meaningful to the type of data we are handling (lecture).\n4. Show the output of the new extracted information.","metadata":{}},{"cell_type":"code","source":"# Answer here\nimport json\nimport os\n\ninput_prompt_b5 = [types.Part(file_data=types.FileData(file_uri='https://www.youtube.com/watch?v=VWnN4J7Zblw'),\n                           video_metadata=types.VideoMetadata(fps=0.1, start_offset=\"6000s\", end_offset=\"6200s\")),\n                types.Part(text=\"Provide the transcript verbatim of everything explained in the video lecture, describing the presentation slides' content as well.\")]\n\nmy_transcript_lecture_yt_vid, my_logs_lecture_yt_vid = prompt_gemini(input_prompt = input_prompt_b5, model_name=\"gemini-2.5-flash\", new_config=new_config, with_parts = True, with_tokens_info = True)\n\nos.makedirs(\"./results/info_extractions\", exist_ok=True)\nmy_json_filepath = \"./results/info_extractions/lecture_analysis_b5.json\" \nmy_struct_yt_lecture_dict = json.loads(my_transcript_lecture_yt_vid)\nwith open(my_json_filepath, \"w\") as f:\n    json.dump(my_struct_yt_lecture_dict, f, indent=4)\n\nmy_lecture_txt = create_langextract_input_from_json(\n    json_filepath=my_json_filepath, \n    output_txt_filepath=\"./results/info_extractions/lecture_for_langextract_b5.txt\"\n)\n\nmy_prompt_lect = prompt_lect\nmy_examples_lect = examples_lect.copy()\n\nnew_class_description = textwrap.dedent(\"\"\"\\\n    Use the 'example_given' class for when the professor provides a concrete example to explain a concept.\n    - 'example_summary': A brief summary of the example.\n    - 'concept_illustrated': The concept that this example is for.\n\"\"\")\nmy_prompt_lect = my_prompt_lect + \"\\n\\n\" + new_class_description\n\nnew_example = lx.data.ExampleData(\n    text=\"To make this clearer, let's think about a real-world scenario. \"\n         \"Imagine you are sorting a physical deck of 52 playing cards. \"\n         \"This is a good way to visualize O(n^2) complexity.\",\n    extractions=[\n        lx.data.Extraction(\n            extraction_class=\"example_given\",\n            extraction_text=\"Imagine you are sorting a physical deck of 52 playing cards.\",\n            attributes={\n                \"example_summary\": \"Sorting a deck of 52 playing cards.\",\n                \"concept_illustrated\": \"O(n^2) complexity\"\n            }\n        )\n    ]\n)\n\nmy_examples_lect.append(new_example)\n\nmy_html_content = grounded_info_extraction(\n    my_lecture_txt, \n    my_prompt_lect,   \n    my_examples_lect,  \n    \"lecture_extraction_bonus5\" \n)\n\nmy_html_content","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-14T09:57:23.769065Z","iopub.execute_input":"2025-11-14T09:57:23.769394Z","iopub.status.idle":"2025-11-14T09:57:53.241846Z","shell.execute_reply.started":"2025-11-14T09:57:23.769377Z","shell.execute_reply":"2025-11-14T09:57:53.240691Z"}},"outputs":[],"execution_count":null}]}